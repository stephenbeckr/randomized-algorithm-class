# Policies and syllabus

APPM/STAT 5650  Randomized Algorithms.  University of Colorado Boulder, Fall 2021.

More class info (grades, discrimination policies, secret zoom link, piazza link, etc.) are on our [Canvas page](https://canvas.colorado.edu/courses/76997) (sign in via the CU SSO)

[Catalog description](https://catalog.colorado.edu/search/?P=APPM%205650): Investigates modern randomized methods that are used in scientific and numerical computing, in particular randomized matrix approximation methods. Other topics may include stochastic gradient methods and variance reduced versions, compressed sensing, and locality sensitive hashing.

Requisites: Restricted to graduate students only.
Recommended: Prerequisite APPM 4440 or equivalent

This class is expected to meet in person, though this may change due to the COVID-19 situation.

## Instructor and contact Information

- The instructor is [Stephen Becker](http://amath.colorado.edu/faculty/becker/), Associate Professor of Applied Mathematics
- Contact him at <stephen.becker@colorado.edu>
- Office: 338 ECOT (engineering center, office tower)
- There are no teaching assistants (TAs) for this course

## Meeting times and location
Meeting time: MWF 10:20 AM - 11:10 AM

Location: ECCR 257 "Newton Lab" (this is in the engineering center, classroom wing)

## Office Hours
3 hours per week, hold in a hybrid fashion: attend the physical office (338 ECOT) or via zoom (link is posted in Canvas)

Times: 

- (Wed office hours are now permanently on Friday, since HW is now due Monday)
- Thu 3 to 5 PM
- Fri 3 to 4 PM

## Estimated Workload
This is intended to have a normal workload for an elective course, lower than the workload for a core course.  This is official a three credit course (the standard kind of course).

## Prereqs
APPM 3310 “Matrix Methods” or similar; but APPM 4440 “Real Analysis” or equivalent (e.g., APPM 5440) is suggested.  Having a course in numerics can be helpful.  Basic familiarity with probability is essential.

## Other
There are no required **textbooks**, nor **recitations**, nor **exams**.  Instead of a final exam, students will turn in a project.

## Projects

There is one project, worth 25% of your grade. The last few weeks, there will be no/less homework and instead there is a final project, which is open-ended. Students are encouraged to form groups (up to three people). It can be either theory or computation. Topics should be discussed with the instructor. The final week, groups will give brief (about 10 min) presentations on their topic.

## Homeworks

There will be weekly homeworks. You are allowed to drop one homework (this will be done automatically).

## Grading
- 75% Homework.
  - Late homework is not accepted, but you are allowed one free "dropped" homework. Your lowest-scoring homework will be automatically dropped. See below for caveat
- 25% Project

The overall grade may be **curved** as appropriate (not guaranteed), but note that there is no set "quota" of A's, B's, etc., so you are not directly competing with your classmates.

**Note**: To get a grade of "A", you **cannot drop any homeworks**.  If you have to drop a homework, then the highest grade you can get is "A-".  (There are no "A+" grades at CU)

## Late assignment and cheating policy

In general, **late homework** assignments are not accepted; instead, you can use your one dropped homework.  Under exceptional circumstances (such as serious illness, including COVID-19, or serious family issues), homework can be turned in late.  If the reason is foreseeable (e.g., planned travel), you must contact the instructor in advance.

Examples:

- Your sister is getting married and you have to travel out-of-state.  That's great, but this is when you use the one dropped homework. This is foreseeable, and not an "emergency", so it does not count as an exceptional circumstance.
- A close family member becomes infected with COVID-19 and you have to return to your home country to take care of family.  This *does* count as an exceptional circumstance. Please email the instructor to discuss arrangements.

**Cheating** is not acceptable.  Take-home exams and homeworks are easy to cheat on if you really want to, but as this is an upper-division course, I am relying on the fact that students are here to learn (and paying the university to do so), and thus cheating does not make sense.  Cheating does not hurt the instructor, it hurts the student (and hurts the grades of honest classmates).

If a student is caught cheating, on the first occurrence, the penalty ranges from losing points on the item in question (like one test problem; this is for very minor infractions) to losing all points for the assignment (i.e., the entire homework or entire exam). Students may be referred to the honor council. On the second occurrence of cheating, similar penalties may apply, and additionally the student may fail the class, at the instructor's discretion.

"Minor infractions" include not following the instructions during an exam (in person or remote). For example, if the instructions on a remote test are to keep your microphone on and your hands in sight of your webcam, then failing to follow these instructions construes a minor infraction, and (even though cheating may not be proven) you are subject to losing points.

On homeworks, you are free to **collaborate** with other students, and to use resources like the internet appropriately. However, you must do your own work. There is a gray area between collaboration and cheating, and we rely on the students' and instructors discretion.  Copying code verbatim is never permissible.  You should be writing up your own work, and explaining answers in your own words.  Snippets of code are allowed to be similar (sometimes there is only one good way to do it), but longer chunks of code should never be identical.  If not expressly forbidden by the assignment, you may use the internet, but you may never post for help on online forums.  (Regarding forums, please use our Piazza website if you want a Q&A forum).

Cheating is not usually an issue in this class, and I have faith that students will continue to act appropriately.



## Course website

We will use [github](https://github.com/stephenbeckr/randomized-algorithm-class) for public content (notes, demos, syllabus), and use CU's default LMT **Canvas** for private content like grades and homework solutions.  Canvas will also be used to organize things, like any recorded lectures, comments made via **gradescope** (if we start using that because the class goes remote), and Q&A forums via **piazza**.

## Online behavior
The class is intended to be in person, but in the event we switch to remote, we will use zoom.

On zoom, please have your webcam on if at all possible

- Valid reasons for not having the camera on: to protect the privacy of your family or roommate, if you cannot have a virtual background
- Invalid reason: you don't feel like it, or you didn't wash your hair.

We have the same standards of behavior as we would in a classroom: appropriate attire, appropriate and not distracting virtual backgrounds, verbal and chat remarks should be respectful, etc.  Real-world backgrounds should be appropriate and professional (please, no drugs or alcohol behind you).

It's always important to have respectful remarks, and even more so in an online setting, since it is easier to get carried away with chat comments since you cannot see the effect on other people.

If we enable private chat on zoom, remember that the zoom host can later see even "private" chats. Inappropriate or inconsiderate remarks, even on private chats, are not allowed.


## Dropping the Course
Advice from your department advisor is recommended before dropping any course. After 11:59 PM Wed Sep. 8, dropping a course results in a "W" on your transcript and you’ll be billed for tuition. After 11:59 PM Fri Oct. 29, dropping the course is possible only with a petition approved by the Dean’s office.

(The last day to *add* a class is Wed Sep. 1)

# Generic policies
For classroom behavior, requirements for COVID-19, accommodation for disabilities, preferred student names and pronouns, honor code, sexual misconduct/discrimination/harassment/retaliation and religious holidays, please refer to the policies document posted on Canvas.


# Syllabus

Randomized methods have been a core part of computer science (CS) for a long time, but are only recently being used in scientific and numerical computing. This course focuses on these recent advances, and is distinguished from more traditional CS “randomized algorithm” courses by its focus on continuous (or at least floating point) math, as opposed to discrete math. The course will also discuss stochastic gradient methods and variance reduced versions like SVRG. Research in the field is ongoing and there is not yet a textbook, and it also means the course is not comprehensive but rather will present a biased selection of topics.

### Official course description
Investigates modern randomized methods that are used in scientific and numerical computing, in particular randomized matrix approximation methods. Other topics may include stochastic gradient methods and variance reduced versions, compressed sensing, and locality sensitive hashing.

### Related courses at CU
This course is **not** similar to the CS department's [CSCI 6214 Randomized Algorithms](https://catalog.colorado.edu/search/?P=CSCI%206214) because that class mainly focuses on discrete algorithms and ["Las Vegas" style randomization](https://en.wikipedia.org/wiki/Las_Vegas_algorithm) as opposed to Monte Carlo style.  This course and CSCI 6214 are *complementary*.


### Principal Topics
computer architecture, tail bounds (Hoeffding, Chernoff, Bernstein), sketches, Johnson-Lindenstrauss Lemma, Fast JLT, CountSketch, sparsification, leverage scores, regresssion, SVD, compressed sensing, stochastic gradient descent, variance reduction, locality sensitive hashing, coresets

### Learning Goals/Outcomes
[//]: # ( Not testable; high-level )
[//]: # ( Learning Objectives, i.e., quantifiable outcomes )
[//]: # ( Something measurable )
After taking this course, students should be familiar with core tools, such as various types of randomized sketches, and applications to various matrix decompositions, least-squares, and simple optimization problems. Students will understand modern stochastic gradient descent, and the use of locality sensitive hashing. Students will be able to evaluate when a randomized approach may be beneficial over classical approaches.


### Programming
Homeworks will involve both mathematical analysis and programming.

Students are expected to already know how to program.  We encourage using **Python** and **Matlab**; **Julia** is another good choice though we will not be using it explicitly.  For homework assignments, usually the deliverable is the outcome of some code, so therefore the student may choose any reasonable programming language. However, we will be doing demonstrations in Python and/or Matlab (and the instructor is best at debugging Python and Matlab).  Most of our demonstrations will be using [github](http://github.com) in conjunction with [python via colab](https://colab.research.google.com/).

For more involved coding projects, we recommend use of an [IDE](https://en.wikipedia.org/wiki/Integrated_development_environment) that supports Python: either a cloud-based one like JupyterLab or [replit.com](https://replit.com/); or traditional software IDEs like [VS Code](https://code.visualstudio.com/), PyCharm or Spyder.

We do **not** recommend using C, C++ or Fortran for programming in this class; these languages take a lot longer to prototype in, and the performance can be fast but only if you do it right (and usually requires calling the right kind of libraries, like [NAG](https://www.nag.com/content/nag-library) or GLPK or using suites like PETSc, Trilinos, [Eigen](http://eigen.tuxfamily.org/index.php?title=Main_Page), etc.)

## Rough list of topics

**Part 1: Preliminaries**

1. How a computer works according to a mathematician
2. Blocked computations, BLAS, memory hierarchy, single-pass methods
3. Applied linear algebra facts, matrix multiplication, norms, Eckart-Young etc.
4. Matrix decompositions: QR, SVD, LU, Cholesky
5. Iterative methods: power, CG, MINRES, etc.
6. Basic probability: Chebyshev, Markov and Bernstein/Chernoff inequalities

**Part 2: Sketches and randomized dimensionality transforms**

1. Gaussian and Haar distribution
2. Johnson-Lindenstrauss Lemma, subspace embeddings
3. Fast JL versions
4. Count sketch
5. Selection via leverage scores

**Part 3: The Good Stuff**

1. Least squares/regression, iterative Hessian Sketch, BLENDENPIK and LSRN 2. SVD decompositions
3. One-pass SVD decompositions
4. Nystrom, CX, CUR and ID decompositions (CSSP)
5. Sketching for PCA
6. Tensor sketching (cf. Malik and Becker)
7. Randomized trace estimators for genomics (cf. Border and Becker)
8. Sketching covariance matrices (cf. Pourkamali-Anaraki and Becker)
9. Stochastic Gradient methods (SGD)
10. Variance reduced SGD, e.g., SVRG, SAGA

**Possible additional topics**

1. Monte Carlo and quasi-Monte Carlo
2. Analysis of K-means++
3. Sketching with conditional gradient/Frank-Wolfe: “sketchy decisions”
4. Smoothed analysis
5. CS-style randomized algorithms (sorting, shortest-path, etc.)

## More info on topics (from previous class, Spring 2019)

The actual topics we covered, and links to references, are on this [google sheet](https://docs.google.com/spreadsheets/d/1z2yT99o8nCiotU0OZbrmmk0kAjff5iUDhKo3fpRVORA/edit?usp=sharing).  See below for a high-level list of what we covered. There was no single textbook for the class (and no standard set of notes).

- Introduction
  - Exact rank-r randomized SVD; randomized sorting
- How a computer works
  - Vector architecture, BLAS, LAPACK, blocking, memory hierarchy, 1-pass methods, sparse formats, HDF5
  - Reference: [Viktor Eijkhout's book](http://pages.tacc.utexas.edu/~eijkhout/istc/istc.html)
- Linear algebra
  - Norms and related inequalities, linear operators and operator norms, inner product on matrices, SVD, spectral and Frobenius norms, unitary invariance, Eckardt-Young, QR decomp.
- Basic probability
  - Conditional probability, expectation, variance, moments, linearity, law of total probability/variance/expectation, Bayes; Boole/union bound, Markov, Chebyshev, Jensen
- Tail bounds
  - Hoeffding/Chernoff/Bernstein
  - Reference: [Vershynin's 2018 book](https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html)
- Sketches, Johnson-Lindenstrauss Lemma
  - Reference for our version of proof: [Kakade and Shakhnarovich's notes](http://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/)
- JL to subspace embeddings via epsilon nets
  - Reference: [Woodruff 2014, Thm 2.3 and nearby](https://arxiv.org/abs/1411.4357)
- Fast Johnson-Lindenstrauss a la Ailon and Chazelle
  - References for detailed topics: [Becker and Pourkamali](http://arxiv.org/abs/1511.00152), [BLENDENPIK paper](https://dl.acm.org/citation.cfm?id=1958633), [Error correcting codes paper](http://proceedings.mlr.press/v37/ubaru15.html), [James' implementation code](https://github.com/jamesfolberth/fast_methods_big_data_project)
- CountSketch, very sparse sketches
  - References: [Clarkson and Woodruff for Countsketch](https://doi.org/10.1145/3019134 2017), [Li, Hastie and Church '06 for very sparse random projections](https://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf)
- Entry-wise sampling
  - References: [Achlioptas, Karnin and Liberty '13 "Simple is Best"](https://pdfs.semanticscholar.org/aa64/b8fb3382e42f90ee93a1dd0c78f13833963f.pdf), and also [Arora, Hazan and Kale '06](https://scholar.google.com/scholar?cluster=15000094124799237010&hl=en&as_sdt=0,6), [Achlioptas,  Karnin and Liberty '13](https://scholar.google.com/scholar?cluster=5902035377314465212&hl=en&as_sdt=0,6)
- Leverage scores, approximate matrix multiplication
  - References: [Mahoney's 2011 monograph](https://scholar.google.com/scholar?cluster=17892460197857789799&hl=en&as_sdt=0,6) and [Mahoney and Drineas 2017 summer school notes](http://arxiv.org/abs/1712.08880)
- Least-squares
  - deterministic backgroun: normal eq'ns, ill-conditioning, 1-pass classical algorithms, Tikhonov, Moore-Penrose pseudo-inverse
  - via sketching, Iterative Hessian Sketch, Blendenpik/LSRN, and randomized Kaczmarz
  - Reference for IHS: [Pilanci and Wainwright](http://www.jmlr.org/papers/volume17/14-460/14-460.pdf)
  - Reference for Blendenpik and LSRN: [Blendenpik paper](https://epubs.siam.org/doi/abs/10.1137/090767911), [LSRN paper](https://epubs.siam.org/doi/abs/10.1137/120866580)
  - Reference for randomized Kaczmarz: [Strohmer and Vershynin '08](http://www.springerlink.com/index/10.1007/s00041-008-9030-4), and extensions for importance sampling and SGD: [Eldar and Needell 2011](https://arxiv.org/abs/1008.4397), [Needell and Tropp 2012](https://arxiv.org/abs/1208.3805), and [Needell, Srebro and Ward 2016](https://arxiv.org/abs/1310.5715)
- l1 regression
  - Cauchy sketch, p-stable random variables
  - Reference: [Woodruff's monograph, ch 3](https://arxiv.org/abs/1411.4357), and [Clarkson et al.'s fast Cauchy transform](http://epubs.siam.org/doi/10.1137/140963698)
- randomized SVD
  - algorithm, one-pass variants, proofs
  - Reference: [Halko, Martinsson and Tropp 2011](https://epubs.siam.org/doi/10.1137/090771806), more practical versions [Tropp, Yurtsever, Udell and Cevher, SIMAX 2017](https://epubs.siam.org/doi/abs/10.1137/17M1111590)
  - Reference for one-pass version: [Yu et al, IJCAI 2017](https://arxiv.org/abs/1704.07669)
- Compressed sensing
  - Overview and proofs, quick slightly sub-optimal proofs of RIP via Johnson-Lindenstrauss
  - Reference: our proofs followed [Rauhut's 2011 monograph](http://www.mathc.rwth-aachen.de/~rauhut/files/LinzRauhut.pdf)
- Matrix Completion/Euclidean Distance Completion
- Monte Carlo
  - Background, background on quadrature
  - Improvements to Monte Carlo (quasi-Monte Carlo and control variates)
  - Reference for quasi-MC: [Dick, Kuo, Sloan; Acta Numerica 2013](https://doi.org/10.1017/S0962492913000044)
- Stochastic Gradient Descent (SGD)
  - Improvements (SAGA, SVRG)
  - Reference: [Bottou, Curtis and Nocedal, Siam Review '18](http://arxiv.org/abs/1606.04838)
- Locality Sensitive Hashing (LSH)
  - Background on hash functions (hash tables, cryptographic hashes)
  - k-NN, MinHash, SimHash, Euclidean distance
  - Reference for LSH: [ch 3 of Rajaraman and Ullman 2010](http://infolab.stanford.edu/~ullman/mmds/ch3a.pdf)
  - Reference for hashing: A classic reference is vol 3 of D. Knuth's 1968 "The Art of Computer Programming", but wikipedia is also convenient
- CountMin sketch and friends
  - min/median non-linear post-processing; AMS sketch; versions of Count sketch
  - Reference: [Cormode's 2011 review](http://www.cs.umass.edu/~mcgregor/711S12/sketches1.pdf) and [Cormode's 2013 Simon's talk](http://dimacs.rutgers.edu/~graham/pubs/html/TalkSimons13.html)
- Coresets
  - For k-means clustering via k-means++
  - Reference: [Bachem, Lucic and Krause '17](https://arxiv.org/abs/1703.06476)

Guest lectures:
- Richard Border about [Stochastic Lanczos quadrature, Border and Becker '19](https://www.biorxiv.org/content/10.1101/607168v1)
- Osman Malik about tensor sketches and interpolative decomposition: [Malik and Becker 2018 NeurIPS](https://papers.nips.cc/paper/8213-low-rank-tucker-decomposition-of-large-tensors-using-tensorsketch) and [Malik and Becker 2019 ID](https://arxiv.org/abs/1901.10559)

# Resources (since there is no main textbook)
Here is a list of monographs that are helpful:

- Gunnar Martinsson previously taught a version of this course, and still has information on the course website [Fast algorithms for big data from spring 2016](https://amath.colorado.edu/faculty/martinss/Teaching/APPM5720_2016s/index.html).
- Gunnar Martinsson also has a 34 page monograph [Randomized methods for matrix computation (started 2016, updated 2018)](http://arxiv.org/abs/1607.01649)
  - He and Joel Tropp also have the 2020 monograph [Randomized Numerical Linear Algebra: Foundations & Algorithms](https://arxiv.org/abs/2002.01387)
- Roman Vershynin published a Nov 2018 300 page book [High-Dimensional Probability: An Introduction with Applications in Data Science](https://www.cambridge.org/gb/academic/subjects/statistics-probability/probability-theory-and-stochastic-processes/high-dimensional-probability-introduction-applications-data-science?format=HB#8FyVeUCeT4PcEjMp.97) published by Cambridge University Press
  - Like many books from Cambridge University Press, they are allowing Vershynin to host a [PDF copy on his website](https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html).
  - A good complement is [Joel Tropp's 2011 "An Introduction to Matrix Concentration Inequalities"](https://arxiv.org/abs/1501.01571)
- David P. Woodruff has a 157 page monograph [Sketching as a Tool for Numerical Linear Algebra](http://dx.doi.org/10.1561/0400000060) from 2014 as part of the Foundations and Trends in Theoretical Computer Science series.
  - You can access the text via the [arXiv version](https://arxiv.org/abs/1411.4357)
- Michael Mahoney has a 100 page monograph [Randomized algorithms for matrices and data](http://dx.doi.org/10.1561/2200000035) from 2011 as part of the Foundations and Trends® in Machine Learning series
  - You can access the text via the [arXiv version](https://arxiv.org/abs/1104.5557)
- [Lectures on Randomized Numerical Linear Algebra, by Michael Mahoney and Petros Drineas, 2017](http://arxiv.org/abs/1712.08880), "This chapter is based on lectures on Randomized Numerical Linear Algebra from the 2016 Park City Mathematics Institute summer school on The Mathematics of Data."
- Alex Gittens teaches [CSCI 6220 Randomized Algorithms](http://www.cs.rpi.edu/~gittea/teaching/fall2018/csci6220-and-4030.html) at RPI and covers good material

## Textbooks for related topics
For numerical linear algebra, see [Applied Numerical Linear Algebra by James Demmel (SIAM 1997)](https://epubs.siam.org/doi/book/10.1137/1.9781611971446?mobileUi=0). You have free access to a PDF version of each chapter if you are on CU campus
