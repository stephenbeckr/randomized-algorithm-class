\RequirePackage[l2tabu, orthodox]{nag}	% tells out-of-date packages
\documentclass[10pt, letterpaper]{scrartcl}

\usepackage{amssymb,amsfonts}
\usepackage{amsthm} % if using theorems and proofs
\usepackage[cmex10]{amsmath}
\usepackage[usenames,dvipsnames,svgnames]{xcolor} % use like \textcolor{red}{..} or {\color{red} ...}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}% can also add 'nohead', or leave blank

%\usepackage{url}

\usepackage{cancel} % for canceling terms

\usepackage{booktabs}

\usepackage{pdfpages} % for including pdfs

\usepackage{algorithm}
\usepackage{algpseudocode}  %algorithmicx
%\usepackage{wrapfig}

%\usepackage{listings}
\usepackage[numbered,framed]{matlab-prettifier}
\lstdefinestyle{mat}{
    frame=single,
    language=matlab,
    showstringspaces=false,
    %  keywordstyle=\bfseries\color{blue},
    %  commentstyle=\color{green},
    %  stringstyle=\color{magenta},
}
\lstset{
    style              = Matlab-editor,
    basicstyle         = \mlttfamily,
    escapechar         = ",
    mlshowsectionrules = true,
}
%% use withL: \lstinputlisting{myODEfunction.m}


\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern} % important!

\usepackage{hyperref}
\hypersetup{pdfauthor={Stephen Becker},
    pdftitle={},
    colorlinks=true,
    citecolor=MidnightBlue,
    urlcolor=Bittersweet,
}
% -- Input macros and such --
\IfFileExists{custom_headers.tex}{
\input{custom_headers}}{
\input{../custom_headers}}{
}
\let\oldenumerate\enumerate
\renewcommand{\enumerate}{
  \oldenumerate
  \setlength{\itemsep}{1em}
  \setlength{\parskip}{1em}
  \setlength{\parsep}{0pt}
}
\usepackage{enumitem}
%\setlist{noitemsep}
% See http://mirrors.fe.up.pt/pub/CTAN/macros/latex/contrib/enumitem/enumitem.pdf
% and now, set par indent to zero
\newlength{\savedparindent}
\setlength{\savedparindent}{\parindent}
\setlist[enumerate]{listparindent=\savedparindent}
%\setlength{\parindent}{0pt} % Jan 2017, HW 3/4, comment this out.

\usepackage{xspace}
%\newcommand{\collaboration}{\textbf{Collaboration Allowed}\xspace}
%\newcommand{\nocollaboration}{\textbf{No Collaboration}\xspace}

\renewcommand{\T}{\mathbb{T}}
\newcommand{\fn}{\widehat{f}_n} % Fourier coefficients
\newcommand{\spi}{\frac{1}{\sqrt{2\pi}}}
\renewcommand{\H}{\mathcal{H}}
\renewcommand{\phi}{\varphi}
\DeclareMathOperator{\ran}{ran} % ker already defined, and with lower case

\newcommand*\Laplace{\mathop{}\!\mathbin\bigtriangleup}
\renewcommand{\SS}{\mathcal{S}} % Schwartz space

% My solution environment
%%\newenvironment{solution}{\setlength{\parindent}{\savedparindent}{\bfseries Solution}:}{}
\makeatletter
\newcommand\solParagraph{\@startsection{paragraph}{4}{\z@}%
%    {-3.25ex \@plus -1ex \@minus -0.2ex}%
    {-.55ex \@plus -1ex \@minus -0.2ex}%
    {0.01pt}%
    {\raggedsection\normalfont\sectfont\nobreak\size@paragraph}%
}
\makeatother
\newenvironment{solution}{\setlength{\parindent}{\savedparindent}\solParagraph{Solution:}}{}

%\newenvironment{solution}{\emph{Solution}:}{}
\newenvironment{instructions}{}{}
\newenvironment{SolnComment}{}{}
\usepackage{comment} 

%\def\solutions{1}  % define solutions  % !!! COMMENT or UNCOMMENT THIS LINE

% NOTE: \begin{solution} should have NO SPACES BEFORE OR AFTER IT,
% 	Can give very weird errors, hiding text, etc. See http://tex.stackexchange.com/a/91431/4603
\ifdefined\solutions
    \newcommand{\solTitle}[1]{#1}
    \excludecomment{instructions}
\else
   \excludecomment{solution}% uncomment this line to hide solution
   \newcommand{\solTitle}[1]{}
   \excludecomment{SolnComment}
\fi


\title{Homework 9 \solTitle{Selected Solutions} \\APPM 4720/5720 Spring 2019 \\ Randomized Algorithms}
%\author{Stephen Becker}
\date{}
\begin{document}
\maketitle
\vspace{-6em}
\textbf{Due date}: Friday, Mar 15 2019

Theme: Randomization for K-means clustering   \hfill Instructor: Stephen Becker %Dr.\ Becker
%\vspace{1em}

\begin{instructions}
\paragraph{Instructions}
Collaboration with your fellow students is allowed and in fact recommended, although direct copying is not allowed.  Please write down the names of the students that you worked with. The internet is allowed for basic tasks only, not for directly looking for solutions.

An arbitrary subset of these questions will be graded.

%\paragraph{Reading} 
%Read the rest of chapters 9--11 in [BV2004]. 
%%Read chapter 3.1, 3.2 and 3.3 in [BV2004].
%%Students are \textbf{strongly advised} to skim appendices A and C in [BV2004] to look for unfamiliar material (and read in more detail if there is unfamiliar material)
%
%%\vspace{1em}
%%\textbf{Special instructions this week}:
%
\end{instructions}


\renewcommand{\xi}{x^{(i)}}
\newcommand{\xj}{x^{(j)}}
\newcommand{\muj}{\mu^{(j)}}
\newcommand{\M}{\mathcal{M}}
\vspace{2ex}
In this homework, we look at the K-means clustering problem and the standard algorithm to solve it (called Lloyd's algorithm, or usually just called the K-means algorithm).
Let $X \in \R^{n \times p}$ be a dataset where each row is an observation, so we have observations $\{ \xi \}_{i=1}^n \subset \R^p$ (so technically each $\xi$ is a  row vector,
but think of it as just a vector). I use this row-wise convention since it matches Matlab's default clustering convention. We let $K$ denote the total number of clusters we'd like to find. In K-means algorithms, a cluster is defined by a cluster center $\muj$, and let $\M = \{\muj\}_{j=1}^K$ be the set of all cluster centers. The clustering/partition of a data point $x$ is then determined by computing
\begin{equation}\label{eq:assignment}
d_x \defeq \min_{ \mu \in\M } \; \|x-\mu\|_2, \quad
j_x \defeq \argmin_{ \mu \in\M } \; \|x-\mu\|_2.
\end{equation}

We will look at a randomized variant known as ``K-means++''\footnote{D. Arthur and S. Vassilvitskii, ``k-means++: The advantages of careful seeding'', SODA 2007} which has had a big impact on machine learning. This method takes only $K$ steps, and produces a set $\M$ which is guaranteed, in expectation, to be not so far from the optimal set of cluster centers; specifically, the expected value of the clustering objective value is within $8(\ln(K)+2)$ times the minimum objective value. This is a big deal because finding the true optimal set of cluster centers is a NP-hard problem, even if $K=2$ (see \footnote{D. Aloise, A. Deshpande, P. Hansen, P. Popat, ``NP-hardness of Euclidean sum-of-squares clustering''. Machine Learning. 75: 245–249, 2009 }) or even if $p=2$ (see \footnote{M. Mahajan, P. Nimbhorkar, K. Varadarajan, The Planar k-means Problem is NP-hard, Lecture Notes in Computer Science. 5431: 274–285, 2009}).

\begin{algorithm}
    \begin{algorithmic}[1]
        \Procedure{K-means++}{$X,K$} \Comment{$X$ is $n\times p$, $K$ is number of clusters} 
        \State $\M \gets \emptyset$ \Comment{The set of cluster centers}
        \State Choose $j \sim \text{Uniform}[1,\ldots,n]$
        \State $\M \gets M \cup \{ \xj \}$
        \For{$k=2,\ldots,K$} %\Comment{We have the answer if r is 0}
        \State Compute $d_i \equiv d_{\xi}$ via \eqref{eq:assignment} for $\xi$, $i=1,\ldots,n$ \Comment{This depends on $\M$ of course}
        \State Choose $j$ at random from a weighted distribution with probabilities $\{d_i^2\}_{i=1}^n$
        \State $\M \gets M \cup \{ \xj \}$
        \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

The very basic idea is that selecting points that are far away from the existing cluster centers is a good way to find new clusters. If this is such a good idea, why not just deterministically pick the one point that is farthest away? Such a short-sighted choice is a \href{https://en.wikipedia.org/wiki/Greedy_algorithm}{``greedy choice''}. We'll formalize that method in the procedure below:

\begin{algorithm}
\begin{algorithmic}[1]
         \Procedure{Greedy K-means}{$X,K$}
         \Statex Steps 2--6 and 8--10 same as K-means++ 
    \makeatletter
    \setcounter{ALG@line}{6}
    \makeatother
       \State  \hspace{9.5mm} Choose $j$ deterministically as the index of the maximal value of $\{d_i^2\}_{i=1}^n$
    \makeatletter
\setcounter{ALG@line}{9}
\makeatother
   \EndProcedure
\end{algorithmic}
\end{algorithm}

Note that in practice, K-means++ is not a full algorithm itself but used to initialize a K-means algorithm (since Lloyd's algorithm can only improve the K-means objective every iteration). For our homework, we'll just stick with the initial K-means++ and not do anything afterwards. 



\begin{enumerate}[align=left, leftmargin=*, label=\sffamily\bfseries Problem \arabic*:]   
 
    \item \ [CODING] Compare the performance of K-means++ and ``Greedy K-means'' (do this over many runs; each run, K-means++ makes new random choices, and Greedy K-means makes a new initial selection for step 3).
    
    Do this on the full MNIST test data, in \texttt{mnist\_data\_all.mat} which is on Canvas.
    This has 4 variables, \texttt{Train} which is $60000 \times 784$ (e.g., $n=60000$ and $p=784=28^2$); 
    \texttt{Test} which is $10000 \times 784$; \texttt{Train\_labels} which is $60000 \times 1$ and contains the true labels of the training data points, so an integer 0--9; and \texttt{Test\_labels} which has the true labels of the testing data points.
    
    Run the algorithms on the training data, for $K=10$, then evaluate error on both testing and training data. Note: evaluate the classification error (the number of correctly classified points), \emph{not} the K-means objective error (though you can do that too if you like).

    
    \textbf{Deliverable}: A plot (e.g., a box-plot aka box-and-whiskers plot) showing the performance of the two methods for both testing and training error on the MNIST data; and, a short discussion of the results.
    
    \emph{Hints}: the amount of coding can be reduced if you use Matlab's \texttt{pdist2} function (requires Stats toolbox), or my faster variant \texttt{pdist2\_faster} available on github. Read the documentation carefully; you may want to get the \emph{squared} Euclidean norm; you can also request the code to output the quantity $j_x$ from \eqref{eq:assignment} which is useful not just in Step 7 of ``Greedy K-means'' but also in evaluating the accuracy of your method on the training and testing data.  In python, with SciPy, see  \href{https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.cdist.html}{scipy.spatial.distance.cdist.html}; or with Scikit-learn, \href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html}{sklearn.metrics.pairwise\_distances.html}.
    
    Perhaps the biggest amount of coding will be to actually evaluate the accuracy of your code. There's one major problem: the true data has labels $(0,1,\ldots,9)$, and your algorithm will give $K=10$ clusters (say, $(a,b,c,d,e,f,g,h,i,j)$), but you don't know what the correspondence between these clusters is. One method to do this, which is optimal under a certain criteria, is the \href{https://en.wikipedia.org/wiki/Hungarian_algorithm}{``Hungarian algorithm''} or ``Munkres'' algorithm. For code to do this in Matlab, see \href{https://github.com/ZJULearning/MatlabFunc/tree/master/Tools}{github.com/ZJULearning/MatlabFunc/tree/master/Tools} and the \texttt{bestMap.m} file (which relies on \texttt{hungarian.m}).  For python, see
    \href{https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.linear_sum_assignment.html}{scipy.optimize.linear\_sum\_assignment.html} if you have SciPy, or if not, then use Scikit-learn's  \href{https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/linear_assignment_.py}{linear\_assignment\_.py}.
    
    
    You may want to run plain K-means on the MNIST data, to get a feel for what to expect. Using Matlab's \texttt{kmeans}, I get about $51\% \pm 2\%$ accuracy on the training and testing data (compared to a benchmark of $10\%$ accuracy for random guessing, since we have 10 classes).  You may also want to randomly subsample the training data to speed up the algorithm; for example, I found that reducing the training data from 10000 to 2000 had very little effect on overall accuracy.
    
    \textbf{Bonus} (not required, not for credit): how does performance change if you select weights based on $\{d_i\}$ instead of $\{d_i^2\}$?
    
    
%\begin{lstlisting}
%\end{lstlisting}
    
    

\end{enumerate}   
\end{document}
